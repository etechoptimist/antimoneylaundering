{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in c:\\users\\hp\\onedrive\\documents\\eduardo toledo\\asai\\antimoneylaundering\\.venv\\lib\\site-packages (2.2.3)\n",
      "Requirement already satisfied: seaborn in c:\\users\\hp\\onedrive\\documents\\eduardo toledo\\asai\\antimoneylaundering\\.venv\\lib\\site-packages (0.13.2)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\hp\\onedrive\\documents\\eduardo toledo\\asai\\antimoneylaundering\\.venv\\lib\\site-packages (3.10.0)\n",
      "Collecting scipy\n",
      "  Downloading scipy-1.15.1-cp312-cp312-win_amd64.whl.metadata (60 kB)\n",
      "Requirement already satisfied: numpy>=1.26.0 in c:\\users\\hp\\onedrive\\documents\\eduardo toledo\\asai\\antimoneylaundering\\.venv\\lib\\site-packages (from pandas) (2.2.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\hp\\onedrive\\documents\\eduardo toledo\\asai\\antimoneylaundering\\.venv\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\hp\\onedrive\\documents\\eduardo toledo\\asai\\antimoneylaundering\\.venv\\lib\\site-packages (from pandas) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\hp\\onedrive\\documents\\eduardo toledo\\asai\\antimoneylaundering\\.venv\\lib\\site-packages (from pandas) (2025.1)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\hp\\onedrive\\documents\\eduardo toledo\\asai\\antimoneylaundering\\.venv\\lib\\site-packages (from matplotlib) (1.3.1)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\hp\\onedrive\\documents\\eduardo toledo\\asai\\antimoneylaundering\\.venv\\lib\\site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\hp\\onedrive\\documents\\eduardo toledo\\asai\\antimoneylaundering\\.venv\\lib\\site-packages (from matplotlib) (4.55.6)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\hp\\onedrive\\documents\\eduardo toledo\\asai\\antimoneylaundering\\.venv\\lib\\site-packages (from matplotlib) (1.4.8)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\hp\\onedrive\\documents\\eduardo toledo\\asai\\antimoneylaundering\\.venv\\lib\\site-packages (from matplotlib) (24.2)\n",
      "Requirement already satisfied: pillow>=8 in c:\\users\\hp\\onedrive\\documents\\eduardo toledo\\asai\\antimoneylaundering\\.venv\\lib\\site-packages (from matplotlib) (11.1.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\hp\\onedrive\\documents\\eduardo toledo\\asai\\antimoneylaundering\\.venv\\lib\\site-packages (from matplotlib) (3.2.1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\hp\\onedrive\\documents\\eduardo toledo\\asai\\antimoneylaundering\\.venv\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Downloading scipy-1.15.1-cp312-cp312-win_amd64.whl (43.6 MB)\n",
      "   ---------------------------------------- 0.0/43.6 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/43.6 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/43.6 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/43.6 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.3/43.6 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.3/43.6 MB ? eta -:--:--\n",
      "    --------------------------------------- 0.8/43.6 MB 1.2 MB/s eta 0:00:38\n",
      "    --------------------------------------- 1.0/43.6 MB 1.1 MB/s eta 0:00:39\n",
      "   - -------------------------------------- 1.8/43.6 MB 1.8 MB/s eta 0:00:24\n",
      "   -- ------------------------------------- 2.6/43.6 MB 2.1 MB/s eta 0:00:20\n",
      "   --- ------------------------------------ 3.9/43.6 MB 2.9 MB/s eta 0:00:14\n",
      "   ------ --------------------------------- 6.6/43.6 MB 4.0 MB/s eta 0:00:10\n",
      "   ------- -------------------------------- 8.7/43.6 MB 4.7 MB/s eta 0:00:08\n",
      "   ------------- -------------------------- 14.7/43.6 MB 7.2 MB/s eta 0:00:05\n",
      "   ---------------- ----------------------- 18.1/43.6 MB 8.0 MB/s eta 0:00:04\n",
      "   ------------------ --------------------- 20.2/43.6 MB 8.7 MB/s eta 0:00:03\n",
      "   ------------------ --------------------- 20.2/43.6 MB 8.7 MB/s eta 0:00:03\n",
      "   ------------------------ --------------- 27.3/43.6 MB 9.4 MB/s eta 0:00:02\n",
      "   --------------------------- ------------ 30.1/43.6 MB 9.8 MB/s eta 0:00:02\n",
      "   --------------------------------- ------ 36.2/43.6 MB 10.9 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 40.9/43.6 MB 11.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------  43.5/43.6 MB 12.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 43.6/43.6 MB 11.5 MB/s eta 0:00:00\n",
      "Installing collected packages: scipy\n",
      "Successfully installed scipy-1.15.1\n"
     ]
    }
   ],
   "source": [
    "\n",
    "!pip install pandas seaborn matplotlib scipy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Session             Start time         Receipt Time   \\\n",
      "0  4067038  08/01/2023 01:01:13 AM  2023-08-01 01:01:13   \n",
      "1  4067039  08/01/2023 01:03:42 AM  2023-08-01 01:03:43   \n",
      "2  4067040  08/01/2023 01:04:16 AM  2023-08-01 01:04:16   \n",
      "3  4067041  08/01/2023 01:05:43 AM  2023-08-01 01:05:44   \n",
      "4  4067046  08/01/2023 01:12:30 AM  2023-08-01 01:12:30   \n",
      "\n",
      "                 End Time       Location    Card/Check No.    First Name   \\\n",
      "0  08/01/2023 01:01:13 AM  Morongo Casino  434256******8498             -   \n",
      "1  08/01/2023 01:03:43 AM  Morongo Casino  443042******7551  KHALIL ABDEL   \n",
      "2  08/01/2023 01:04:16 AM  Morongo Casino  551915******8420             -   \n",
      "3  08/01/2023 01:05:44 AM  Morongo Casino  515676******1782    JOSE JULIO   \n",
      "4  08/01/2023 01:12:30 AM  Morongo Casino  440066******8319        MI JIN   \n",
      "\n",
      "   Last Name   VIP Player Name          Status   Processor   Trans Type   \\\n",
      "0           -                 -  Debit Dispense          DS        Debit   \n",
      "1      RAHMAN                 -        Approved          WP        Debit   \n",
      "2           -                 -  Debit Dispense          DS        Debit   \n",
      "3       ARIAS                 -        Approved          WP       Credit   \n",
      "4        PARK          11586119        Approved          WP       Credit   \n",
      "\n",
      "   Card/Check Type    Receipt No.    Amount    Fee    Total   \\\n",
      "0              VISA      6063565.0       300   15.0    315.0   \n",
      "1              VISA      6063566.0       200   10.0    210.0   \n",
      "2        MasterCard      6063567.0        40    2.0     42.0   \n",
      "3        MasterCard      6063568.0       200   12.0    212.0   \n",
      "4              VISA      6063569.0       500   30.0    530.0   \n",
      "\n",
      "           Cashier    Err/Dec    Auth   \n",
      "0               NaN        0.0     NaN  \n",
      "1   Logan McCannon         0.0  846976  \n",
      "2               NaN        0.0     NaN  \n",
      "3     Martha Dimas         0.0  06342Z  \n",
      "4  Cindy Echeverri         0.0  09082B  \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Index(['Session', 'Start time ', ' Receipt Time ', ' End Time', ' Location ',\n",
       "       ' Card/Check No. ', ' First Name ', ' Last Name ', ' VIP Player Name ',\n",
       "       ' Status ', ' Processor ', ' Trans Type ', ' Card/Check Type ',\n",
       "       ' Receipt No. ', ' Amount ', ' Fee ', ' Total ', ' Cashier ',\n",
       "       ' Err/Dec ', ' Auth '],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Read the CASTransactionSummary CSV file\n",
    "df = pd.read_csv('CASTransactionSummary.csv')\n",
    "\n",
    "# Display the first few rows of the dataframe\n",
    "print(df.head())\n",
    "df.columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "from scipy import stats\n",
    "\n",
    "def analyze_transaction_patterns(df):\n",
    "    \"\"\"\n",
    "    Create baseline statistics for normal transaction patterns.\n",
    "    \n",
    "    Parameters:\n",
    "    df: DataFrame with columns [TransactionDate, TransactionType, AccountNumber, Amount]\n",
    "    \n",
    "    Returns:\n",
    "    dict with baseline statistics\n",
    "    \"\"\"\n",
    "   \n",
    "    df['TransactionDate'] = pd.to_datetime(df[' End Time'])\n",
    "    df['Hour'] = df['TransactionDate'].dt.hour\n",
    "    df['DayOfWeek'] = df['TransactionDate'].dt.dayofweek\n",
    "  \n",
    "    df.rename(columns={' Card/Check No. ': 'account_number', \n",
    "                       ' Amount ': 'amount', \n",
    "                       ' Trans Type ': 'trans_type'}, \n",
    "              inplace=True)\n",
    " \n",
    "    \n",
    "    # Enhanced account-level statistics\n",
    "    account_stats = {\n",
    "        # Basic transaction patterns\n",
    "        'tx_count': df.groupby('account_number').size().describe(),\n",
    "        'max_amount': df.groupby('account_number')['amount'].max().describe(),\n",
    "        \n",
    "        # New: Additional statistical measures\n",
    "        'amount_skewness': df.groupby('account_number')['amount'].agg(stats.skew).describe(),\n",
    "        'amount_kurtosis': df.groupby('account_number')['amount'].agg(stats.kurtosis).describe(),\n",
    "        'tx_entropy': df.groupby('account_number')['trans_type'].agg(lambda x: stats.entropy(x.value_counts())).describe(),\n",
    "     \n",
    "        \n",
    "        # Time patterns\n",
    "        'common_hours': df.groupby('Hour').size().sort_values(ascending=False),\n",
    "        'common_days': df.groupby('DayOfWeek').size().sort_values(ascending=False),\n",
    "        \n",
    "        # Transaction type distribution\n",
    "        'tx_type_dist': df['trans_type'].value_counts(normalize=True),\n",
    "        \n",
    "   }\n",
    "    \n",
    "    # Amount patterns\n",
    "    avg_amount = df.groupby('account_number')['amount'].mean()\n",
    "    \n",
    "    account_stats['avg_amount'] = {\n",
    "        'mean': avg_amount.mean(),\n",
    "        'std': avg_amount.std(),\n",
    "        'percentiles': avg_amount.quantile([0.25, 0.5, 0.75, 0.95, 0.99])\n",
    "    }\n",
    "    # Enhanced velocity metrics\n",
    "    velocities = {\n",
    "        '1H': df.groupby(['account_number', pd.Grouper(key='TransactionDate', freq='1h')]).size(),\n",
    "        '24H': df.groupby(['account_number', pd.Grouper(key='TransactionDate', freq='24h')]).size(),\n",
    "        '7D': df.groupby(['account_number', pd.Grouper(key='TransactionDate', freq='7D')]).size()\n",
    "    }\n",
    "    \n",
    "    account_stats['velocity_metrics'] = {\n",
    "        period: {\n",
    "            'mean': vel.mean(),\n",
    "            'std': vel.std(),\n",
    "            'percentiles': vel.quantile([0.25, 0.5, 0.75, 0.95, 0.99])\n",
    "        }\n",
    "        for period, vel in velocities.items()\n",
    "    }\n",
    "    \n",
    "    return account_stats\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\HP\\OneDrive\\Documents\\Eduardo Toledo\\ASAI\\antimoneylaundering\\.venv\\Lib\\site-packages\\pandas\\core\\groupby\\generic.py:324: RuntimeWarning: Precision loss occurred in moment calculation due to catastrophic cancellation. This occurs when the data are nearly identical. Results may be unreliable.\n",
      "  f = lambda x: func(x, *args, **kwargs)\n",
      "c:\\Users\\HP\\OneDrive\\Documents\\Eduardo Toledo\\ASAI\\antimoneylaundering\\.venv\\Lib\\site-packages\\pandas\\core\\groupby\\generic.py:324: RuntimeWarning: Precision loss occurred in moment calculation due to catastrophic cancellation. This occurs when the data are nearly identical. Results may be unreliable.\n",
      "  f = lambda x: func(x, *args, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'tx_count': count    51366.000000\n",
       " mean         2.841685\n",
       " std          4.793049\n",
       " min          1.000000\n",
       " 25%          1.000000\n",
       " 50%          1.000000\n",
       " 75%          3.000000\n",
       " max        192.000000\n",
       " dtype: float64,\n",
       " 'max_amount': count    51366.000000\n",
       " mean       554.989662\n",
       " std        844.900156\n",
       " min          7.000000\n",
       " 25%        120.000000\n",
       " 50%        300.000000\n",
       " 75%        500.000000\n",
       " max      20000.000000\n",
       " Name: amount, dtype: float64,\n",
       " 'amount_skewness': count    20141.000000\n",
       " mean         0.131779\n",
       " std          0.689780\n",
       " min         -4.694855\n",
       " 25%          0.000000\n",
       " 50%          0.000000\n",
       " 75%          0.506408\n",
       " max          8.553677\n",
       " Name: amount, dtype: float64,\n",
       " 'amount_kurtosis': count    20141.000000\n",
       " mean        -1.194952\n",
       " std          1.506675\n",
       " min         -2.000000\n",
       " 25%         -2.000000\n",
       " 50%         -1.500000\n",
       " 75%         -0.960000\n",
       " max         75.135978\n",
       " Name: amount, dtype: float64,\n",
       " 'tx_entropy': count    51366.000000\n",
       " mean         0.031858\n",
       " std          0.134265\n",
       " min          0.000000\n",
       " 25%          0.000000\n",
       " 50%          0.000000\n",
       " 75%          0.000000\n",
       " max          0.693147\n",
       " Name: trans_type, dtype: float64,\n",
       " 'common_hours': Hour\n",
       " 22    9607\n",
       " 21    9176\n",
       " 23    9149\n",
       " 20    9125\n",
       " 19    8618\n",
       " 0     8300\n",
       " 18    8249\n",
       " 17    8209\n",
       " 16    8083\n",
       " 15    7874\n",
       " 1     7432\n",
       " 14    7182\n",
       " 13    6296\n",
       " 2     6296\n",
       " 12    5292\n",
       " 3     4872\n",
       " 11    4099\n",
       " 4     3479\n",
       " 10    3311\n",
       " 5     2615\n",
       " 9     2552\n",
       " 6     2235\n",
       " 8     2032\n",
       " 7     1883\n",
       " dtype: int64,\n",
       " 'common_days': DayOfWeek\n",
       " 6    34002\n",
       " 5    31241\n",
       " 4    19676\n",
       " 0    16354\n",
       " 3    15426\n",
       " 1    14752\n",
       " 2    14515\n",
       " dtype: int64,\n",
       " 'tx_type_dist': trans_type\n",
       " Debit     0.541215\n",
       " Credit    0.436465\n",
       " Check     0.022320\n",
       " Name: proportion, dtype: float64,\n",
       " 'avg_amount': {'mean': np.float64(459.0443777981833),\n",
       "  'std': np.float64(684.9692708109964),\n",
       "  'percentiles': 0.25     100.0\n",
       "  0.50     250.0\n",
       "  0.75     500.0\n",
       "  0.95    1500.0\n",
       "  0.99    3000.0\n",
       "  Name: amount, dtype: float64},\n",
       " 'velocity_metrics': {'1H': {'mean': np.float64(1.2514553700798203),\n",
       "   'std': np.float64(0.6943209382591947),\n",
       "   'percentiles': 0.25    1.0\n",
       "   0.50    1.0\n",
       "   0.75    1.0\n",
       "   0.95    3.0\n",
       "   0.99    4.0\n",
       "   dtype: float64},\n",
       "  '24H': {'mean': np.float64(1.5529289103559802),\n",
       "   'std': np.float64(1.2004310658125812),\n",
       "   'percentiles': 0.25    1.0\n",
       "   0.50    1.0\n",
       "   0.75    2.0\n",
       "   0.95    4.0\n",
       "   0.99    6.0\n",
       "   dtype: float64},\n",
       "  '7D': {'mean': np.float64(1.6932035681557183),\n",
       "   'std': np.float64(1.4832656934146273),\n",
       "   'percentiles': 0.25    1.0\n",
       "   0.50    1.0\n",
       "   0.75    2.0\n",
       "   0.95    4.0\n",
       "   0.99    8.0\n",
       "   dtype: float64}}}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "baseline_stats=analyze_transaction_patterns(df)\n",
    "baseline_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_amount_suspicious(transaction, df, baseline_stats):\n",
    "    # Get account's typical behavior\n",
    "    account_history = df[df['account_number'] == transaction['account_number']].copy()\n",
    "    \n",
    "    # Check if we have enough transactions for meaningful statistics\n",
    "    if len(account_history) >= 2:\n",
    "        # Use robust statistics less sensitive to non-normal distributions\n",
    "        median_amount = account_history['amount'].median()\n",
    "        q1 = account_history['amount'].quantile(0.25)\n",
    "        q3 = account_history['amount'].quantile(0.75)\n",
    "        iqr = q3 - q1\n",
    "    else:\n",
    "        # Use baseline statistics if not enough account history\n",
    "        median_amount = baseline_stats['avg_amount']['percentiles'][0.5]  # median\n",
    "        q1 = baseline_stats['avg_amount']['percentiles'][0.25]\n",
    "        q3 = baseline_stats['avg_amount']['percentiles'][0.75]\n",
    "        iqr = q3 - q1\n",
    "    \n",
    "    # Use Modified Z-score with median and IQR instead of mean and std\n",
    "    # This is more robust for non-normal distributions\n",
    "    if iqr == 0:\n",
    "        # If IQR is 0, fall back to percent difference from median\n",
    "        percent_difference = abs((transaction['amount'] - median_amount) / median_amount) if median_amount != 0 else float('inf')\n",
    "        is_suspicious = percent_difference > 0.5  # Flag if difference is more than 50%\n",
    "        score = percent_difference\n",
    "        method = 'percent_difference'\n",
    "    else:\n",
    "        # Calculate modified z-score using median and IQR\n",
    "        # 0.6745 is used because for normal distributions, IQR/0.6745 â‰ˆ std\n",
    "        modified_score = 0.6745 * (transaction['amount'] - median_amount) / iqr\n",
    "        is_suspicious = abs(modified_score) > 3.5  # Slightly higher threshold than regular z-score\n",
    "        score = modified_score\n",
    "        method = 'modified_z_score'\n",
    "    \n",
    "    if is_suspicious:\n",
    "        alert = {\n",
    "            'account': transaction['account_number'],\n",
    "            'alert_type': 'suspicious_amount',\n",
    "            'subclass': f'{method}: {score:.2f}',\n",
    "            'transaction': transaction,\n",
    "            'reference_stats': {\n",
    "                'median': median_amount,\n",
    "                'q1': q1,\n",
    "                'q3': q3,\n",
    "                'iqr': iqr\n",
    "            }\n",
    "        }\n",
    "        return (True, alert)\n",
    "    else:\n",
    "        return (False, None)       \n",
    "def is_velocity_suspicious(transaction, account_history):\n",
    "    \"\"\"\n",
    "    Calculate z-scores for transaction velocities in different time windows,\n",
    "    handling cases where there's no historical data for the transaction period\n",
    "    \"\"\"\n",
    "    # Ensure TransactionDate is datetime\n",
    "    account_history['TransactionDate'] = pd.to_datetime(account_history['TransactionDate'])\n",
    "    tx_date = pd.Timestamp(transaction['TransactionDate'])\n",
    "    \n",
    "    # Create a temporary DataFrame including the new transaction\n",
    "    new_tx_df = pd.DataFrame([{\n",
    "        'TransactionDate': tx_date,\n",
    "        'Amount': transaction['amount']\n",
    "    }])\n",
    "    \n",
    "    # Combine with historical data\n",
    "    combined_history = pd.concat([account_history, new_tx_df])\n",
    "    combined_history = combined_history.sort_values('TransactionDate')\n",
    "    \n",
    "    velocities = {\n",
    "        '1h': combined_history.groupby(pd.Grouper(key='TransactionDate', freq='1h')).size(),\n",
    "        '24h': combined_history.groupby(pd.Grouper(key='TransactionDate', freq='24h')).size(),\n",
    "        '7D': combined_history.groupby(pd.Grouper(key='TransactionDate', freq='7D')).size()\n",
    "    }\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    for window, velocity_series in velocities.items():\n",
    "        # Get current period\n",
    "        current_period = tx_date.floor(window)\n",
    "        \n",
    "        # Calculate mean and std of historical velocities (excluding current period)\n",
    "        historical_velocities = velocity_series[velocity_series.index < current_period]\n",
    "        \n",
    "        if len(historical_velocities) > 0:\n",
    "            mean_velocity = historical_velocities.mean()\n",
    "            std_velocity = historical_velocities.std()\n",
    "        else:\n",
    "            # No historical data for this time window\n",
    "            mean_velocity = 0\n",
    "            std_velocity = 0\n",
    "        \n",
    "        # Get current period's velocity\n",
    "        current_velocity = velocity_series.get(current_period, 1)  # At least 1 for the new transaction\n",
    "        \n",
    "        # Calculate z-score\n",
    "        if std_velocity == 0:\n",
    "            # If no variation in historical data or no historical data\n",
    "            z_score = float('inf') if current_velocity > mean_velocity else 0\n",
    "        else:\n",
    "            z_score = (current_velocity - mean_velocity) / std_velocity\n",
    "        \n",
    "        results[window] = {\n",
    "            'z_score': z_score,\n",
    "            'is_suspicious': abs(z_score) > 3,\n",
    "            'current_velocity': current_velocity,\n",
    "            'mean_velocity': mean_velocity,\n",
    "            'std_velocity': std_velocity,\n",
    "            'has_history': len(historical_velocities) > 0\n",
    "        }\n",
    "    \n",
    "    return results\n",
    "\n",
    "def flag_unusual_transactions(transaction, df, baseline_stats):\n",
    "    \"\"\"\n",
    "    Flag potentially unusual transactions based on simple rules\n",
    "    \"\"\"\n",
    "    alerts = []\n",
    "    account_number = \"Card/Check No.\"\n",
    "    # Amount thresholds (using 95th percentile)\n",
    "    amount_threshold = baseline_stats['avg_amount']['percentiles'][0.95]\n",
    "    \n",
    "    # Velocity threshold (using 95th percentile)\n",
    "    velocity_24h_threshold = baseline_stats['velocity_metrics']['24H']['percentiles'][0.95]\n",
    "    velocity_1h_threshold = baseline_stats['velocity_metrics']['1H']['percentiles'][0.95]\n",
    "    velocity_7d_threshold = baseline_stats['velocity_metrics']['7D']['percentiles'][0.95]\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    # Check for unusual amounts\n",
    "    high_amount_tx = transaction['Amount'] > amount_threshold\n",
    "        \n",
    "    # Check for unusual velocity\n",
    "    new_tx_df = pd.DataFrame([transaction])\n",
    "    new_tx_df['End Time'] = pd.to_datetime(new_tx_df['TransactionDate'])\n",
    "   \n",
    "    account_df = df[df[account_number] == transaction[account_number]].copy()\n",
    "    account_df = pd.concat([account_df, new_tx_df], ignore_index=True)\n",
    "   \n",
    "    # Sort by TransactionDate to ensure proper time-based grouping\n",
    "    account_df = account_df.sort_values('End Time')\n",
    "    \n",
    "    counts_24h = account_df.groupby(pd.Grouper(key='End Time', freq='24h')).size()\n",
    "    high_velocity_24h = counts_24h[counts_24h > velocity_24h_threshold]\n",
    "    \n",
    "    counts_1h = account_df.groupby(pd.Grouper(key='End Time', freq='1h')).size()\n",
    "    high_velocity_1h = counts_1h[counts_1h > velocity_1h_threshold]\n",
    "                    \n",
    "    counts_7d = account_df.groupby(pd.Grouper(key='End Time', freq='7D')).size()\n",
    "    high_velocity_7d = counts_7d[counts_7d > velocity_7d_threshold]\n",
    "        \n",
    "        \n",
    "    if high_amount_tx:\n",
    "            alerts.append({\n",
    "                'account': transaction[account_number],\n",
    "                'alert_type': 'high_amount',\n",
    "                'transactions': transaction\n",
    "            })\n",
    "            \n",
    "    if not high_velocity_24h.empty:\n",
    "        tx_date = pd.Timestamp(transaction['TransactionDate'])\n",
    "        tx_period = tx_date.floor('24h')\n",
    "        \n",
    "        if tx_period in high_velocity_24h.index:\n",
    "            alerts.append({\n",
    "                'account': transaction[account_number],\n",
    "                'alert_type': 'high_velocity_24h',\n",
    "                'dates': high_velocity_24h.index\n",
    "            })  \n",
    "    \n",
    "    \n",
    "    if not high_velocity_1h.empty:\n",
    "        tx_date = pd.Timestamp(transaction['TransactionDate'])\n",
    "        tx_period = tx_date.floor('1h')\n",
    "        \n",
    "        if tx_period in high_velocity_1h.index:\n",
    "            alerts.append({\n",
    "                'account': transaction[account_number],\n",
    "                'alert_type': 'high_velocity_1h',\n",
    "                'dates': high_velocity_1h.index\n",
    "            })\n",
    "    \n",
    "    if not high_velocity_7d.empty:\n",
    "        # Convert transaction date to the same frequency as the grouper (7D)\n",
    "        tx_date = pd.Timestamp(transaction['TransactionDate'])\n",
    "        # Find the period containing the transaction date\n",
    "        tx_period = tx_date.floor('7D')\n",
    "        \n",
    "        if tx_period in high_velocity_7d.index:\n",
    "            alerts.append({\n",
    "                'account': transaction[account_number],\n",
    "                'alert_type': 'high_velocity_7d',\n",
    "                'dates': high_velocity_7d.index\n",
    "            })\n",
    "            \n",
    "    return alerts\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(True, {'account': '481582******3305', 'alert_type': 'suspicious_amount', 'subclass': 'modified_z_score: 133.55', 'transaction': {'account_number': '481582******3305', 'amount': 20000, 'TransactionDate': '2023-07-13 01:01:13'}, 'reference_stats': {'median': np.float64(200.0), 'q1': np.float64(100.0), 'q3': np.float64(200.0), 'iqr': np.float64(100.0)}})\n",
      "{'1h': {'z_score': np.float64(-0.43627011331319965), 'is_suspicious': np.False_, 'current_velocity': np.int64(28), 'mean_velocity': np.float64(38.12152777777778), 'std_velocity': np.float64(23.200140162962533), 'has_history': True}, '24h': {'z_score': np.float64(-0.94443653818143), 'is_suspicious': np.False_, 'current_velocity': np.int64(618), 'mean_velocity': np.float64(913.3333333333334), 'std_velocity': np.float64(312.7085001412754), 'has_history': True}, '7D': {'z_score': np.float64(-11.86517492887753), 'is_suspicious': np.True_, 'current_velocity': 1, 'mean_velocity': np.float64(6176.0), 'std_velocity': np.float64(520.430590953299), 'has_history': True}}\n"
     ]
    }
   ],
   "source": [
    "transaction = {\n",
    "    'account_number':'481582******3305',\n",
    "    'amount':20000,\n",
    "    'TransactionDate': '2023-07-13 01:01:13'\n",
    "}\n",
    "\n",
    "alert = is_amount_suspicious( transaction, df, baseline_stats)\n",
    "print(alert)\n",
    "result = is_velocity_suspicious(transaction, df)\n",
    "print(result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
